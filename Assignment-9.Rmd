---
title: "Assignment-9"
author: "ItsmeHJB"
date: "01/12/2021"
output: html_document
---

```{r setup, include=FALSE}
library(Stat2Data)
library(tidyverse)
library(MASS)

data("Hawks")
```

## Basic concepts in classification

1. Classification rule - A rule which is able to map a feature vector to categorical values.

2. Learning algorithm - An algorithm which machines can use to learn to classify data.

3. Training data - Data used to train a model.

4. Feature vector - Vector containing feature x1 to xn.

5. Label - Each set of features is assigned one label.

6. Test error - The number of incorrect labels created by the model in comparison to the actual labels in the test data.

7. Train error - The number of incorrect labels created by the modes in comparison to the actual labels in the training data.

8. Train test split - Splitting the original data set into a training and test set.

9. Linear classifier - Cuts the feature space into two with a linear hyper-plane.

## Train test split

Get data frame containing weight, wing, hallux, tail and species. This should only include sharp-shinned or cooper's species.

```{r}
# Data pre-processing
hawks_total <- Hawks %>%
  select(Weight, Wing, Hallux, Tail, Species) %>% 
  filter(Species != "RT") %>% 
  drop_na() %>% 
  mutate(Species=as.numeric(Species=="SS"))

# 60% training data
num_total <- nrow(hawks_total)
num_train <- floor(num_total * 0.60)
num_test <- num_total - num_train

set.seed(1)
test_indi <- sample(seq(num_total), num_test)
train_indi <- setdiff(seq(num_total), test_indi)

# get train and test data
train_hawks <- hawks_total %>% 
  filter(row_number() %in% train_indi)

test_hawks <- hawks_total %>% 
  filter(row_number() %in% test_indi)

# Extract features and labels column
train_x <- train_hawks %>% select(!Species)
train_y <- train_hawks %>% pull(Species)

test_x <- test_hawks %>% select(!Species)
test_y <- test_hawks %>% pull(Species)

# Get most common label from training data
y <- round(sum(train_y) / num_train)

train_error <- mean(abs(y - train_y))
train_error
test_error <- mean(abs(y - test_y))
test_error
```

Using a dumb classifier which simply selects the most common label found in the training data, we get an test error of around 20% for the test data, and 22% for the training data. As a baseline this is interesting as it should be impossible to get worse than this using statistical methods.

From this, we can say that the data sets are relatively different sizes. Data set of type y=1, which is the Sharp-Shinned species, make up about 80% of the total data set.

## Linear discriminant analysis

Linear discriminant analysis is a method which assumes each class of data belongs to a select distribution of data, typically Gaussian. The model will then use conditional probability to select whether the data belongs to each class, with the highest probability class being selected.

This method is considered an application of Bayes Theorem for classification. Using this method to minimise the test error over all possible classifiers.

Below is an example of an LDA trying to predict whether a row of data belongs to a Sharp-shinned or Cooper's species of hawk based off their 4d feature vector.

```{r}
lda_model <- lda(Species ~ ., data = train_hawks)

# Get vector of predictions using training data
lda_train_predictions <- predict(lda_model, train_x)$class %>% 
  as.character() %>% as.numeric()

lda_train_error <- mean(abs(lda_train_predictions - train_y))
lda_train_error

# Get vector of predictions using test data
lda_test_predictions <- predict(lda_model, test_x)$class %>% 
  as.character() %>% as.numeric()

lda_test_error <- mean(abs(lda_test_predictions - test_y))
lda_test_error
```

Using this model, we can see that our predictions of the training data falls close to 4.6% error, whilst the test data is closer to 1.5%. Both of these are obviously much better than just crudly picking 1 every time as before.

## Logistic Regerssion

This method of learning is used to understand the relationship between a dependent variable and one or more independent variable by estimating the probabilities using a regressive approach.

This process in underpinned by two ideas.  
1. We only want to model $P(Y=y | X=x)$  
2. Use the sigmoid function: $S : R \rightarrow (0,1)$ to map real numbers to probabilities.

This essentially predicts the chance of an outcome given individual characteristics.

