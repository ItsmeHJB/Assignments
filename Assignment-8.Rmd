---
title: "Assignment-8"
author: "ItsmeHJB"
date: "24/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Obstacles to valid scientific inference

1. Measurement distortions
 - Measurement which off by a margin in relation to their actual values. This is commonly found in human measurements.
 - An example of this is within a set of manual scales. The head positioning can have a large effect on the read value.
 
2. Selection bias
 - When data in the analysis misrepresents the underlying population. This can manifest itself in a variety of ways: Sample, Self-selection, Attrition or Post-hoc bias.
 - An example of this is within restaurant reviewers. Those that complete reviews must decide that they wish to complete reviews.

3. Confounding variables
 - When trying to find the link between two variables, it is common to find an underlying cause which may be causing the correlation to occur.
 - Comparing ice-cream and sunglasses sales on a day. They correlate together, but the confounding variable is the weather of the day instead.
 
## Unpaired t-test

We wish to replicate the function of: ``` t.test(body_mass_g~species, data=peng_AC,var.equal = TRUE ```

```{r unpaired setup}
library(palmerpenguins)
data("penguins")

peng_AC <- penguins %>%
  drop_na(species, body_mass_g) %>%
  filter(species != "Gentoo")
```

```{r unpaired function}
# Data is data frame
# val_col is string: column name of continuous data column
# group_cal is string: column name of binary variables
t_test_function <- function(data, val_col, group_col, var_equal) {
  # Get unique group_val values
  bin_vars <- data %>%
    pull(group_col) %>% 
    unique()
  
  # Get mean, sd and n for data
  mean_0 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[1]) %>%
    pull(val_col) %>%
    mean()
  
  sd_0 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[1]) %>%
    pull(val_col) %>%
    sd()
  
  n_0 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[1]) %>%
    nrow()
  
  mean_1 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[2]) %>%
    pull(val_col) %>%
    mean()
  
  sd_1 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[2]) %>%
    pull(val_col) %>%
    sd()
  
  n_1 <- peng_AC %>% 
    filter(.data[[group_col]]==bin_vars[2]) %>%
    nrow()
  
  # t_test
  if(var_equal) {
    sd_combined <- sqrt(((n_0-1) * sd_0^2 + (n_1-1) * sd_1^2) / (n_0+n_1-2))
    
    dof <- n_0+n_1-2
    
    t_stat<- (mean_0 - mean_1) / (sd_combined * sqrt(1/n_0+1/n_1))
  
    effect_size <- (mean_0 - mean_1) / sd_combined
  }
  else {  # Welch's test
    t_stat <- (mean_0 - mean_1) / (sqrt((sd_0^2 / n_0) + (sd_1^2 / n_1)))
    
    dof <- ((sd_0^2/n_0)+(sd_1^2/n_1))^2 / (((sd_0^2/n_0)^2 / n_0-1) + ((sd_1^2/n_1)^2 / n_1-1))
  }
  

  p_val <- 2*(1-pt(abs(t_stat), df=dof))
  
  result <- data.frame(t_stat, dof, p_val)
  
  return(result)
}

t_test_function(data=peng_AC, val_col="body_mass_g", group_col="species",var_equal=TRUE)

t_test_function(data=peng_AC, val_col="body_mass_g", group_col="species",var_equal=FALSE)
```

## Statistical hypothesis testing

1. Null hypothesis - The default position where there is no significant difference between two samples.

2. Alternative hypothesis - Opposite to the null hypothesis, this is when there is a significant difference between two samples.

3. Test statistic - A numerical summary of a data-set that can be used to compare and thus determine the outcome of a hypothesis test.

4. Type 1 error - A rejection of a true null hypothesis. Falsely infers the existence of a hypothesis.

5. Type 2 error - Failure to reject a false null hypothesis. Falsely inferring there is a no phenomenon.

6. The size of a test - Probability of a type 1 error under the null hypothesis.

7. The power of a test - Probability of a type 2 error under the alternative hypothesis.

8. The significance level - The upper bound on the test size. The size of the test is always less than this. Typically value $\alpha = 0.05$

9. p-value - Probability that the test statistic is at least as extreme the observed value under the null hypothesis.

10. Effect size - Size of phenomena observed.

Note: the p-value is not the probability that the null hypothesis is true, it is instead the probability that the test statistic is at least as extreme as the observed value **under** the null hypothesis.

When the p-value is less than or equal to the significance level, we reject the null hypothesis as the data appears to favour the alternative.

## Investing test size for an unpaired Student's t-test

```{r unpaired Students t test}
# Set up vars
num_trials<-10000
sample_size<-30
mu_0<-1
mu_1<-1
sigma_0<-3
sigma_1<-3
alpha<-0.05

set.seed(0) # set random seed for reproducibility

# Constant alpha, increasing trial size
single_alpha_test_size_simulation_df<-data.frame(trial=seq(num_trials))%>%
  mutate(sample_0=map(.x=trial,.f=~rnorm(n=sample_size,mean=mu_0,sd=sigma_0)),
         sample_1=map(.x=trial,.f=~rnorm(n=sample_size,mean=mu_1,sd=sigma_1)))%>%
  # generate random Gaussian samples
  mutate(p_value=pmap(.l=list(trial,sample_0,sample_1),
                      .f=~t.test(..2,..3,var.equal = TRUE)$p.value))%>%
  # generate p values - check against alpha to see if type_1_error occurs
  mutate(type_1_error=p_value<alpha)

# Getting mean number of times that p_value < alpha, which is the prob that type_1_error occurs
single_alpha_test_size_simulation_df%>%
  pull(type_1_error)%>%
  mean() # estimate of coverage probability
```

## The power of an unpaired test

```{r}
um_trials<-10000
n_0<-30
n_1<-30
mu_0<-3
mu_1<-4
sigma_0<-2
sigma_1<-2
alpha<-0.05

set.seed(0) # set random seed for reproducibility

data.frame(trial=seq(num_trials))%>%
  mutate(sample_0=map(.x=trial,.f=~rnorm(n=n_0,mean=mu_0,sd=sigma_0)),
         sample_1=map(.x=trial,.f=~rnorm(n=n_1,mean=mu_1,sd=sigma_1)))%>%
  # generate random Gaussian samples
  mutate(p_value=pmap(.l=list(trial,sample_0,sample_1),
                      .f=~t.test(..2,..3,var.equal = TRUE)$p.value))%>%
  # generate p values
  mutate(reject_null=p_value<alpha)%>%
  pull(reject_null)%>%
  mean() # estimate of coverage probability
```


